\section{Part-of-speech Induction}
\label{sec:related}

There are several good reviews of algorithms for unsupervised part of
speech induction
\cite{Christodoulopoulos:2010:TDU:1870658.1870714,Gao:2008:CBE:1613715.1613761}
and models of syntactic category acquisition \cite{ambridge2011child}.

This work is to be distinguished from supervised part of speech
disambiguation systems, which use labeled training data
\cite{Toutanova:2003:FPT:1073445.1073478}, unsupervised disambiguation
systems, which use a dictionary of possible tags for each word
\cite{yatbaz-yuret:2010:POSTERS}, or prototype driven systems which
use a small set of prototypes for each class
\cite{Haghighi:2006:PLS:1220835.1220876}.  The problem of induction is
important for studying under-resourced languages that lack labeled
corpora and high quality dictionaries.  It is also essential in
modeling child language acquisition because every child manages to
induce syntactic categories without access to labeled sentences,
labeled prototypes, or dictionary constraints.

We group the unsupervised POS induction literature under two titles:
(1) type clustering (2) token clustering models.  Type clustering
models strictly cluster all instances of a word type into the same
cluster thus employing one-tag-per-word assumption from the beginning.
Token clustering models cluster tokens into seperate clusters or
enforces them to be in same clusters however these enforcing does not
neccessarily leads to the type clustering.

\subsection{Type Clustering Models}

Type clustering models fall into three subgroups based on the context
representations in Section \ref{sec:representation}.  

\paragraph{Syntagmatic Representation}  
These methods represent the context using the neighboring words and
clusters context vector of word types.  They may suffer from the data
sparsity caused by the infrequent words and the infrequent contexts.
The solutions suggested either restrict the set of words and set of
contexts to be clustered to the most frequently observed, or use
dimensionality reduction.  Sch\"utze
\shortcite{Schutze:1995:DPT:976973.976994} and Lamar et
al. \shortcite{lamar-EtAl:2010:Short} employ SVD to enhance similarity
between less frequently observed word types and contexts.  Redington
et al. \shortcite{redington1998distributional} define context
similarity based on the number of common frames bypassing the data
sparsity problem but achieve lower scores than the best performing
systems.  Maron et al. \shortcite{maron2010sphere} extend the model of
\cite{globerson2007euclidean} by constructing a low dimensional
representation of the data that represents the empirical co-occurrence
statistics of word types and achieve .688 \% many-to-one accuracy on
the PTB, which is covered in more detail in Section~\ref{sec:code}.
Lamar et al. \shortcite{Lamar:2010:LCU:1870658.1870736} represent each
context by the currently assigned left and right tag (which eliminates
data sparsity) and cluster word types using a soft k-means style
iterative algorithm.  They report the best clustering result to date
of .708 many-to-one accuracy on the PTB.

\paragraph{HMMs}

Brown et al. \shortcite{Brown:1992:CNG:176313.176316} use an HMM model
where each type is generated by a latent category and each latent
category depends only on the previous one.  Christodoulopoulos et
al. \shortcite{Christodoulopoulos:2010:TDU:1870658.1870714} show that
the older one-tag-per-word models such as
\cite{Brown:1992:CNG:176313.176316} outperform the more sophisticated
sparse prior
\cite{goldwater-griffiths:2007:ACLMain,johnson:2007:EMNLP-CoNLL2007}
and posterior regularization methods
\cite{Ganchev:2010:PRS:1859890.1859918} both in speed and accuracy
(the Brown model gets .68 many-to-one accuracy on the PTB).  Given
that 93.69\% of the word occurrences in human labeled data are tagged
with their most frequent part of speech
\cite{Toutanova:2003:FPT:1073445.1073478}, this is probably not
surprising; one-tag-per-word is a fairly good first approximation for
induction.  One problem with the previous algorithms in this section
is the poverty of their input features.  Of the syntactic, semantic,
and morphological information linguists claim underlie syntactic
categories, context vectors or bitag HMMs only represent limited
syntactic information in their input.  Experiments incorporating
morphological and orthographic features into HMM based models
demonstrate significant improvements.
\cite{Clark:2003:CDM:1067807.1067817,blunsom-cohn:2011:ACL-HLT2011}
incorporate similar orthographic features and report improvements of
3, and 10\% respectively over the baseline Brown model.

\paragraph{Paradigmatic representations}

Yatbaz et al. \shortcite{yatbaz-sert-yuret:2012:EMNLP-CoNLL} explore
the paradigmatic representation of the word contexts by modeling the
co-occurrence of words and their substitutes within the CODE
framework.  Their experiments on the PTB types shows that paradigmatic
representation improves the state-of-the-art \mto\ and V-measure (\vm)
accuracies to \fmto and \ftvm, respectively.  This paper builds on
that preliminary work by (1) exploring induction of part-of-speechs at
token level (in addition to type level), (2) improving the model for
using additional features, and (3) experimenting with additional
languages.

