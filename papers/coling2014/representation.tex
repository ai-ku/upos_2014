\subsection{Representing Word Context}
\label{sec:representation}

% example substitute vectors (both syntactic and semantic)

In this section we demonstrate the different contextual representations in the
part-of-speech induction (aka. syntactic word categorization) literature and
introduce the substitute words as an alternative to the current context
representations.  In the rest of the paper the words in the vocabulary are
referred as {\em types} and the instances of types are referred as {\em
instances}.

The contextual representations can be categorized into three groups
based on the way they incorporate the local context information of the
target type or instance: (1) syntagmatic representation, (2) Hidden
Markov Models (HMM) and (3) paradigmatic representation.  These
representations can be further subdivided into two subgroups based on
whether they group the types or the instances.

%\subsection{Co-occurrences}
\paragraph{Syntagmatic Representation}

In syntagmatic representation the context is defined with the
neighboring words, typically co-occurrences with a single word on the
left or a single word on the right word called a ``frame'' (e.g., {\em
  {\bf the} dog {\bf is}; {\bf the} cat {\bf is}})
\cite{SchutzePe93,redington1998distributional,mintz2003frequent,20674613,lamar-EtAl:2010:Short,maron2010sphere}.
Turney and Pantel \shortcite{DBLP:journals/jair/TurneyP10} give a
broad overview of syntagmatic approaches and their applications within
the Vector Space Modeling framework.  Depending on the way they
incorporate co-occurences, these models can perform hard (type based)
or soft (instance based) clustering.

Sch\"{u}tze \shortcite{SchutzePe93} represented the context of a word
type by concatenating its left and right co-occurrence vectors.  These
vectors were calculated for each type by using the left and the right
neighbors of the type instances therefore they characterize the
distribution of the left and right neighboring instances of the type.
One constraint of this representation is that it represents types
rather than instances thus it is not possible to group the instances of
any type into the separate categories.

Mintz \shortcite{mintz2003frequent} showed on a subset of child
directed speech corpus (CHILDES) \cite{macwhinney2000childes} that
non-adjacent high frequent bigram frames are useful for the language
learners on the syntactic categorization of the intances.  For example,
the intances that are observed at ``\_'' in the frame ``{\em {\bf the}
  \_ {\bf is}}'' are assigned to the same category.  Using the top-45
frequent frames Mintz achieved an average of 98\% unsupervised
accuracy\footnote{Unsupervised accuracy was defined as the number of
  hits (when two intervening instances that observed in the frame are
  from the same category) divided by number of false alarms (when two
  intervening instances that observed in the frame are from different
  categories).}.  The main limitation of the top-45 frequent frames is
that they could only analyze the 6\% of the instances on average due to
the sparsity.  Another drawback is that the instances with only one
common neighbors could not exchange information.

St Clair et al.  \shortcite{20674613} extended the work of Mintz
\shortcite{mintz2003frequent} and introduced the flexible bigram
frames which represent the context by using the left and the right
bigrams separately.  As a result instances with a common left or right
bigram can exchange information and might be grouped together.  For
instance, two instances that are observed at ```\_'' in ``{\em {\bf the}
  \_ {\bf is}}'' and ``{\em {\bf a} \_ {\bf is}}'' can be categorized
together due to the shared right bigram ``{\em{\bf is}}''.  Using a
feed forward connectionist model they showed that the flexible frames
are statistically better than the frequent frames in terms of the
supervised accuracy\footnote{In order to perform meaningful
  comparisons they used all of the frequent frames instead of the
  top-45 ones.}.  They also showed that representing instance contexts
only with the left or the right bigram is statistically better than
the frequent frames but worse than the flexible frames in terms of
supervised accuracy.  Both Mintz \shortcite{mintz2003frequent} and St
Clair \shortcite{20674613} did not report any results with contexts
larger than bigram since as the context is enriched, the re-occurrence
frequency of a frame becomes lower which causes the data sparsity
\cite{manning99foundations}.

%\subsection{HMM}
\paragraph{HMM} 
Prototypical HMM uses a bigram structure where instances are generated by
latent categories and learns the latent category sequence that
generates the given word sequence instead of clustering instances
directly
\cite{Brown:1992:CNG:176313.176316,blunsom-cohn:2011:ACL-HLT2011,goldwater-griffiths:2007:ACLMain,johnson:2007:EMNLP-CoNLL2007,Ganchev:2010:PRS:1859890.1859918,bergkirkpatrick-klein:2010:ACL,Lee:2010:STU:1870658.1870741}.
The POS induction literature focused on the first and second order
HMMs since the higher order HMMs have additional complicating
factors\footnote{The number of parameters in a prototypical HMM
  quadratically increases as the HMM order increases.}  and require
more complex training procedures \cite{johnson:2007:EMNLP-CoNLL2007}.
Depending on the design and the training procedure HMM models can
group types or instances which are detailed in Section \ref{sec:related}.

%\subsection{Substitute words}
\paragraph{Paradigmatic Representation} 

In the paradigmatic representation context is defined as the distribution of
the substitute words in that context.  Sch\"{u}tze
\shortcite{Schutze:1995:DPT:976973.976994} incorporates paradigmatic
information by concatenating the left co-occurrence and the right co-occurrence
vectors of the right and the left intances, respectively and grouped the instances
that have similar vectors.  The vectors from the neighbors include potential
substitutes.  Yatbaz et al. \shortcite{yatbaz-sert-yuret:2012:EMNLP-CoNLL}
calculate the most likely substitute words of a word in a given context and
clusters the types that have similar substitutes.

Our paradigmatic representation is related to substitute distributions used in
\cite{yatbaz-sert-yuret:2012:EMNLP-CoNLL}.  This method improves on their
foundation from two aspects: (1) it can cluster instances and (2) it models each
features separately.  Moreover we apply our model on 19 corpora in 15
languages. 

Similarly, Sch{\"u}tze and Pedersen \shortcite{SchutzePe93} define the
words that frequently co-occur together as the {\em syntagmatic
  associates} and words that have similar left and right neighbors as
the {\em paradigmatic parallels}.  We find that representing the
paradigmatic axis more directly using substitute vectors rather than
frequent neighbors improves part of speech induction.

Sahlgren \shortcite{sahlgren2006word} gives a detailed analysis of
paradigmatic and syntagmatic relations in the context of word-space
models used to represent the word meanings.  Sahlgren's paradigmatic
model represents word types using co-occurrence counts of their
frequent neighbors, in contrast to his syntagmatic model that
represents word types using counts of contexts (documents, sentences)
they occur in.  Our substitute vectors do not represent word types at
all, but {\em contexts of word instances} using probabilities of likely
substitutes.  Sahlgren finds that in word-spaces built by frequent
neighbor vectors, more nearest neighbors share the same part of speech
compared to word-spaces built by context vectors.


